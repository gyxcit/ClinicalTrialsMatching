# Agent AI Framework

Un framework d'agent IA modulaire et extensible avec support des outils et gestion de la m√©moire, bas√© sur LiteLLM.

## üìã Fonctionnalit√©s

- **Agent intelligent** avec support multi-tours de conversation
- **Syst√®me d'outils** extensible avec appels de fonctions
- **Gestion de la m√©moire** pour contexte conversationnel
- **Support multi-mod√®les** via LiteLLM (Groq, OpenAI, Ollama, etc.)
- **Gestion d'erreurs** robuste avec logging
- **Type hints** complets pour meilleure IDE support

## üèóÔ∏è Architecture

```
code/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ Agent.py          # Agent principal avec boucle de r√©flexion
‚îÇ   ‚îú‚îÄ‚îÄ ModelClient.py    # Client LLM via LiteLLM
‚îÇ   ‚îú‚îÄ‚îÄ memory.py         # Gestion de l'historique
‚îÇ   ‚îú‚îÄ‚îÄ tools.py          # D√©finition et registry des outils
‚îÇ   ‚îú‚îÄ‚îÄ get_trials.py     # R√©cup√©ration donn√©es clinicaltrials.gov
‚îÇ   ‚îî‚îÄ‚îÄ config.py         # Configuration centralis√©e
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ agent_test.py     # Tests de l'agent
‚îú‚îÄ‚îÄ examples/
‚îÇ   ‚îî‚îÄ‚îÄ example_usage.py  # Exemples d'utilisation
‚îî‚îÄ‚îÄ requirements.txt      # D√©pendances
```

## üöÄ Installation

1. Cloner le repository
2. Installer les d√©pendances :

```powershell
pip install -r requirements.txt
```

3. Cr√©er un fichier `.env` avec vos cl√©s API :

```env
GROQ_API_KEY=your_groq_api_key_here
```

## üìñ Utilisation

### Agent Basique

```python
from src.Agent import Agent
import os

# Initialiser l'agent
agent = Agent(
    name="assistant",
    model="groq/llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY")
)

# Envoyer un message
response = agent.chat("Quelle heure est-il ?")
print(response)
```

### ModelClient Direct

```python
from src.ModelClient import ModelClient
import os

# Client pour appels directs au LLM
client = ModelClient(
    model="groq/llama-3.3-70b-versatile",
    api_key=os.getenv("GROQ_API_KEY")
)

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "Hello!"}
]

response = client.generate_response(messages)
print(response["choices"][0]["message"]["content"])
```

### Ajouter des Outils Personnalis√©s

```python
from src.tools import register_tool

def my_custom_tool(param1: str, param2: int) -> str:
    """Ma fonction personnalis√©e."""
    return f"R√©sultat: {param1} x {param2}"

# Enregistrer l'outil
register_tool(
    name="my_tool",
    function=my_custom_tool,
    description="Description de mon outil",
    parameters={
        "type": "object",
        "properties": {
            "param1": {"type": "string", "description": "Premier param√®tre"},
            "param2": {"type": "integer", "description": "Second param√®tre"}
        },
        "required": ["param1", "param2"]
    }
)
```

### Gestion de la M√©moire

```python
from src.memory import Memory

# Cr√©er une m√©moire avec limite
memory = Memory(max_history=10)

# Ajouter des messages
memory.add("user", "Bonjour")
memory.add("assistant", "Salut! Comment puis-je vous aider?")

# R√©cup√©rer l'historique
history = memory.get()
last_5 = memory.get(last_n=5)

# Vider la m√©moire
memory.clear()
```

## üîß Configuration

### Mod√®les Support√©s

- **Groq**: `groq/llama-3.3-70b-versatile`, `groq/mixtral-8x7b-32768`
- **OpenAI**: `openai/gpt-4o`, `openai/gpt-4-turbo`
- **Ollama**: `ollama/mistral:latest`, `ollama/llama2:latest`
- Et tous les mod√®les support√©s par [LiteLLM](https://docs.litellm.ai/docs/providers)

### Options de l'Agent

```python
agent = Agent(
    name="assistant",                           # Nom de l'agent
    system_prompt="You are a helpful AI.",      # Prompt syst√®me
    model="groq/llama-3.3-70b-versatile",      # Mod√®le √† utiliser
    api_key="your_key",                         # Cl√© API
    api_base=None,                              # URL de base (optionnel)
    max_tool_iterations=5                       # Max d'it√©rations d'outils
)
```

## üìä R√©cup√©ration de Donn√©es Cliniques

```python
from src.get_trials import fetch_trials

# R√©cup√©rer des essais cliniques
trials = fetch_trials(
    condition="diabetes",
    max_studies=100,
    return_status=True
)

# Sauvegarder en JSON
fetch_trials(
    condition="diabetes",
    max_studies=100,
    return_status=False,
    json_output=True,
    output_name="diabetes_trials"
)
```

## üß™ Tests

```powershell
python -m test.agent_test
```

## üìù Logging

Le framework utilise le module `logging` standard de Python. Pour configurer :

```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

## üõ†Ô∏è D√©veloppement

### Structure des Outils

Les outils suivent le sch√©ma OpenAI Function Calling :

```python
{
    "type": "function",
    "function": {
        "name": "tool_name",
        "description": "What the tool does",
        "parameters": {
            "type": "object",
            "properties": {
                "param_name": {
                    "type": "string",
                    "description": "Parameter description"
                }
            },
            "required": ["param_name"]
        }
    }
}
```

## üìÑ Licence

Ce projet est sous licence MIT.

## ü§ù Contribution

Les contributions sont les bienvenues ! N'h√©sitez pas √† ouvrir une issue ou une pull request.

## üìö Documentation Compl√©mentaire

- [LiteLLM Documentation](https://docs.litellm.ai/)
- [OpenAI Function Calling](https://platform.openai.com/docs/guides/function-calling)
- [ClinicalTrials.gov API](https://clinicaltrials.gov/api/gui)
